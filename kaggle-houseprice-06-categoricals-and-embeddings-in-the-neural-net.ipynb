{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques (Kaggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 06-categoricals-in-the-neural-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "* Kaggle competition: https://www.kaggle.com/c/house-prices-advanced-regression-techniques\n",
    "* Check missing values (Will Koehrsen): https://www.kaggle.com/willkoehrsen/start-here-a-gentle-introduction by Will Koehrsen\n",
    "* Neural net implementation in PyTorch with embeddings (Yashu Seth): https://yashuseth.blog/2018/07/22/pytorch-neural-network-for-tabular-data-with-categorical-embeddings/ \n",
    "* Neural network embeddings explained (Will Koehrsen) https://towardsdatascience.com/neural-network-embeddings-explained-4d028e6f0526\n",
    "* Sklearn pipelines: https://medium.com/dunder-data/from-pandas-to-scikit-learn-a-new-exciting-workflow-e88e2271ef62\n",
    "* Pipelines with dataframes (John Ramey): https://ramhiser.com/post/2018-04-16-building-scikit-learn-pipeline-with-pandas-dataframe/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Previous**:\n",
    "\n",
    "**kaggle-houseprice-01-linear-model-and-continuous-imputation.ipynb**\n",
    "We try to predict house prices based on a number of continuous and categorical variables.\n",
    "In the first step, the prediction will be made using only a small selection of continuous variables:\n",
    "\n",
    "* LotFrontage: Linear feet of street connected to property\n",
    "* LotArea: Lot size in square feet\n",
    "* 1stFlrSF: First Floor square feet\n",
    "* 2ndFlrSF: Second floor square feet\n",
    "* TotalBsmtSF: Total square feet of basement area\n",
    "* SalePrice: target variable\n",
    "\n",
    "We will use a very simple network: a linear network with a single non-linearity.\n",
    "\n",
    "**kaggle-houseprice-02-data-scaling.ipynb**\n",
    "\n",
    "In order to make it a little easier for gradient descent to converge to a minimum, we will scale the input data to have 0 mean and a standard deviation of 1. For a discussion on why it is useful to scale input data, see https://stats.stackexchange.com/questions/249378/is-scaling-data-0-1-necessary-when-batch-normalization-is-used. We will not scale the target data, following this discussion: https://stats.stackexchange.com/questions/111467/is-it-necessary-to-scale-the-target-value-in-addition-to-scaling-features-for-re.\n",
    "\n",
    "**kaggle-houseprice-03-one-hot-for-missing-continuous.ipynb**\n",
    "\n",
    "Instead of just replacing missing values in our dataset with the mean or the median of the respective column, we will now create a *one-hot encoded vector* to mark the previously *missing data* and add it to the data set. For the same reason that we used the *sklearn.preprocessing StandardScaler* we will now make use of the *sklearn.impute Imputer* to replace missing values. Also, to make this part of data processing a little easier to reuse, we will refactor the code into a function. \n",
    "\n",
    "* missing_LotFrontage: one-hot vector with 1 for each missing value in LotFrontage and 0 else\n",
    "\n",
    "**kaggle-houseprice-04-pipeline-for-preprocessing.ipynb**\n",
    "\n",
    "Instead of relying on self-written code for processing our continuous variables we will now delegate this part of the processing to sklearn transformers. Additionally, those transformers will be put in a pipeline so that the transformers don't have to be called individually every time. This will help keeping our code simple and clean, and produce consistent results for processing multiple data.\n",
    "\n",
    "* Add categorical variables\n",
    "* Extend pipeline to handle categoricals\n",
    "* Create a function to pre-process an arbitrary amount of dataframes at once\n",
    "\n",
    "We still need to add more data to our model. In contrast to the first set of continuous variables, this time we will add categorical variables. Categorical variables differ from continuous variables in the fact that there may or may not be a natural order to values of a categorical variable, and that we cannot use categorical variables to do meaningful calculations (e.g. to calculate the mean, or a sum). For more information see https://en.wikipedia.org/wiki/Level_of_measurement.\n",
    "Often those variables are represented by strings. In order to let our network handle categorical variables, we need to convert them to numbers (also called *factors* or *codes*). Additionally, we will expand our pre-processing pipeline to also handle missing values for categorical variables. We will also create a function that let's us use the pipeline on an arbitrary amount of dataframes at the same time.\n",
    "\n",
    "New variables:\n",
    "* MSZoning: Identifies the general zoning classification of the sale.\n",
    "* MSSubClass: Identifies the type of dwelling involved in the sale.\n",
    "\n",
    "**Now:**\n",
    "\n",
    "* Using categorical variables in the neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show more rows and columns in the pandas output\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "#pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def show_missing(df, show_all=True):\n",
    "    \"\"\"    \n",
    "    Shows absolute and relative number of missing values for each column of a dataframe,\n",
    "    show_all=True also shows columns with no missing values.\n",
    "    \"\"\"\n",
    "    mis_val_abs = df.isnull().sum()\n",
    "    mis_val_rel = df.isnull().sum()/df.shape[0]\n",
    "    mis_val_table = pd.concat([df.dtypes, mis_val_abs, mis_val_rel], axis=1)\n",
    "    mis_val_table = mis_val_table.rename(columns={0: 'dtype', 1: 'Missing abs', 2: 'Missing rel'})\n",
    "\n",
    "    if show_all:\n",
    "        # Sort table descending by relative amount missing\n",
    "        mis_val_table = mis_val_table.sort_values('Missing rel', ascending=False).round(3)\n",
    "    else:\n",
    "        # Sort table descending by relative amount missing, remove columns where no values are missing\n",
    "        mis_val_table = mis_val_table[mis_val_table.iloc[:, 1] != 0].sort_values('Missing rel', ascending=False).round(3)\n",
    "    \n",
    "    return mis_val_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The TypeSelector selects data from a dataframe based on its dtype. Credits to John Ramey, see sources on top.\n",
    "class TypeSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, dtype):\n",
    "        self.dtype=dtype\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        assert isinstance(X, pd.DataFrame)\n",
    "        return X.select_dtypes(include=[self.dtype])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_df(cont_names, cat_names, *dataframes):\n",
    "    \"\"\"\n",
    "    Pre-process arbitrary amount of dataframes with continuous and categorical variables.\n",
    "    The respective fits are being calculated by combining all dataframes into a single\n",
    "    dataframe.\n",
    "    Returns one processed dataframe for each input dataframe.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cont_names : list\n",
    "        List of column names for continuous variables.\n",
    "\n",
    "    cat_names : list\n",
    "        List of column names for categorical variables.\n",
    "\n",
    "    *dataframes : pandas DataFrame(s)\n",
    "        DataFrames to be processed.\n",
    "    \"\"\"\n",
    "    \n",
    "    df_combo = pd.DataFrame(columns=dataframes[0].columns)\n",
    "    for arg in dataframes:\n",
    "        df_combo = pd.concat([df_combo, arg], axis=0, sort=False)\n",
    "        arg[cont_names] = arg[cont_names].astype('float64')\n",
    "        arg[cat_names] = arg[cat_names].astype('category')\n",
    "    \n",
    "    # Convert columns in cont_names to *float64* dtype and the columns of cat_names to *category*.\n",
    "    # This is necessary so that the TypeSelector in the pipeline can differentiate between cont and cat variables.\n",
    "    # The pipeline can then apply different behaviour, according to the dtype.\n",
    "    df_combo[cont_names] = df_combo[cont_names].astype('float64')\n",
    "    df_combo[cat_names] = df_combo[cat_names].astype('category')\n",
    "    \n",
    "    # First, get names of columns with missing values.\n",
    "    # The pipeline below then takes numeric features, in the order of appearance in the input dataframe.\n",
    "    # The pipeline then takes categorical features in the order of appearance in the input dataframe.\n",
    "    # All of these names are then merged to a list, and for the resulting dataframes.\n",
    "    # This naming step is necessary because sklearn does not natively support pandas dataframes, and therefore\n",
    "    #   all column names would be lost otherwise.\n",
    "    missing_names = [f'mis_{name}' for name in df_combo.columns if df_combo[name].isnull().any()]\n",
    "    ordered_cont_names = [col for col in df_combo.columns if col in cont_names]\n",
    "    ordered_cat_names = [col for col in df_combo.columns if col in cat_names]\n",
    "    names = missing_names + ordered_cont_names + ordered_cat_names\n",
    "    \n",
    "    preprocessing_pipeline = make_pipeline(\n",
    "        FeatureUnion(transformer_list=[\n",
    "            ('missing_features', make_pipeline(\n",
    "                MissingIndicator(missing_values=np.nan)\n",
    "            )),\n",
    "            ('numeric_features', make_pipeline(\n",
    "                TypeSelector('float64'),\n",
    "                SimpleImputer(strategy='median'),\n",
    "                StandardScaler()\n",
    "            )),\n",
    "            ('categorical_features', make_pipeline(\n",
    "                TypeSelector('category'),\n",
    "                SimpleImputer(strategy='most_frequent'),\n",
    "                OrdinalEncoder()\n",
    "            ))\n",
    "        ])\n",
    "    )\n",
    "    preprocessing_pipeline.fit(df_combo)\n",
    "        \n",
    "    return (pd.DataFrame(preprocessing_pipeline.transform(arg), columns=names) for arg in dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/houseprice/')\n",
    "#!dir {PATH}  # For Windows\n",
    "!ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training data\n",
    "dep = ['SalePrice']\n",
    "df_train = pd.read_csv(PATH/'train.csv', sep=',', header=0,\n",
    "                       usecols=['MSZoning', 'MSSubClass', 'LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF',\n",
    "                                'TotalBsmtSF', 'SalePrice'])\n",
    "df_y = df_train[dep]\n",
    "df_train = df_train.drop(dep, axis=1)\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data\n",
    "df_test = pd.read_csv(PATH/'test.csv', sep=',', header=0,\n",
    "                       usecols=['MSZoning', 'MSSubClass', 'LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF',\n",
    "                                'TotalBsmtSF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define continuous and categorical columns\n",
    "cat_names = ['MSZoning', 'MSSubClass']\n",
    "cont_names = ['LotFrontage', 'LotArea', '1stFlrSF', '2ndFlrSF', 'TotalBsmtSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we take a look at a couple of rows and some descriptive statistics. This gives us an idea about the scale of values, and helps to decide if some continuous variables should perhaps be treated as categorical. In this case all variables will be treated as continuous.\n",
    "\n",
    "We also check for missing values. If we find any, we have two options: remove the rows that contain missing values (which might lead to losing a lot of observations), or replace them with other values so that the network can use them. Common values used as a replacement are the mean or the median of the series, or some constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train[cont_names].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical variables can be of type int or string. To show all cat columns in describe,\n",
    "# we need to convert them to the same dtype\n",
    "df_train[cat_names].astype('category').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([show_missing(df_train), show_missing(df_test)], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = proc_df(cont_names, cat_names, df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine cat_names to include the new missing value columns\n",
    "cat_names = [col for col in list(df_train.columns) if col not in cont_names]\n",
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([show_missing(df_train), show_missing(df_test)], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make the categorial variables distinct from the continuous variables when they're being used in the model, we will now split categorical and continuous variables for the training dataset.\n",
    "\n",
    "Further, categorical variables will take the datatype torch.long, as opposed to torch.float32 for continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert all data containers to tensors\n",
    "t_train_cat = torch.tensor(df_train[cat_names].values, dtype=torch.long, device=device)\n",
    "t_train_cont = torch.tensor(df_train[cont_names].values, dtype=torch.float32, device=device)\n",
    "t_y = torch.tensor(df_y.values, dtype=torch.float32, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "train_ds = TensorDataset(t_train_cat, t_train_cont, t_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "batch_size=64\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to get the best out of categorical variables is to add them to the neural net by using embedding matrices.\n",
    "The values in the categorical column vector are then just used as a lookup index of the respective embedding matrix.\n",
    "The embedding matrices themselves will be initialized when the model is instantiated, just like any other parameter matrix, and updated during back propagation. This gives the neural net the chance to learn a richer representation about our categorical variables compared to just treating it like any other continuous variable.\n",
    "For more discussion see the sources on top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, emb_dims, num_cont):\n",
    "        super().__init__()        \n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        emb_dims : List of two element tuples\n",
    "            This list will contain a two element tuple for each\n",
    "            categorical feature. The first element of a tuple will\n",
    "            denote the number of unique values of the categorical\n",
    "            feature. The second element will denote the embedding\n",
    "            dimension to be used for that feature.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_cont = num_cont\n",
    "        self.num_embeddings = sum([d for f, d in emb_dims])\n",
    "        self.num_features = self.num_embeddings + self.num_cont\n",
    "        \n",
    "        # Embedding layers       \n",
    "        if self.num_embeddings != 0:        \n",
    "            self.emb_layers = nn.ModuleList(\n",
    "                [nn.Embedding(f, d) for f, d in emb_dims]\n",
    "            )            \n",
    "        \n",
    "        # Layers\n",
    "        self.linear1 = nn.Linear(self.num_features, 100)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(100, 1)        \n",
    "    \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        if self.num_embeddings != 0:\n",
    "            x = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(self.emb_layers)]\n",
    "            x = torch.cat(x, 1)\n",
    "        \n",
    "        if self.num_cont != 0:\n",
    "            x = torch.cat([x, x_cont], 1)\n",
    "        \n",
    "        x = self.linear1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_dims(df):\n",
    "    \"\"\"\n",
    "    Returns a list of tuples of the number of factors of a categorical\n",
    "    variable and the minimum of half that number of factors + 1, and 50.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "    \"\"\"\n",
    "    return [(df[name].nunique(), min(50, (df[name].nunique()+1)//2)) for name in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Embeddings step-by-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is based on the fastai impementation of embedding layers, see https://github.com/fastai/fastai/blob/master/fastai/tabular/models.py#L6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's get the embedding dimensions for our embedding layers.\n",
    "\n",
    "We determine the number of columns of each embedding layer based on the number of factors (that is, the number of unique values of a categorical variable) such that `num_cols = min( (num_factors+1)//2, 50)`.\n",
    "This means that for the boolean column *Mis_MSZoning* (the result of the MissingIndicator above), the number of factors is 2 (since we have only 0 or 1 as possible values), and therefore the number of columns for this embedding layer is the minimum of (3//2, 50), which is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# If we're checking for the total number of factors, we need to consider all data\n",
    "num_factors = pd.concat([df_train['mis_MSZoning'], df_test['mis_MSZoning']], axis=0).nunique()\n",
    "num_cols = min((num_factors+1)//2, 50)\n",
    "num_factors, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's check for MSZoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_factors = pd.concat([df_train['MSZoning'], df_test['MSZoning']], axis=0).nunique()\n",
    "num_cols = min((num_factors+1)//2, 50)\n",
    "num_factors, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far, so good! Now we can do it for all categorical variables at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb_dims = get_emb_dims(pd.concat([df_train[cat_names], df_test[cat_names]], axis=0, sort=False))\n",
    "emb_dims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The total number of columns over all embedding layers will give us the number of input values needed for categorical variables in the first linear layer of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_embeddings = sum([y for x, y in emb_dims])\n",
    "num_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's manually define the embedding layers based on the dimensions we calculated before. PyTorch offers the `nn.Embedding` class to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb_layer0 = nn.Embedding(2, 1)\n",
    "emb_layer1 = nn.Embedding(2, 1)\n",
    "emb_layer2 = nn.Embedding(2, 1)\n",
    "emb_layer3 = nn.Embedding(16, 8)\n",
    "emb_layer4 = nn.Embedding(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Once instantiated, the embedding layers will also have random weights already initialized. For an alternative initialization, we can manually re-initialize the layers, but we will skip that for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# torch.nn.init.kaiming_normal_(emb_layer.weight)\n",
    "emb_layer0.weight, emb_layer1.weight, emb_layer2.weight, emb_layer3.weight, emb_layer4.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, we can combine all embedding layers into a `nn.ModuleList`, which is basically a set of layers for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb_layers = nn.ModuleList([emb_layer0, emb_layer1, emb_layer2, emb_layer3, emb_layer4])\n",
    "emb_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The values of the categorical variables will be used as a lookup index for corresponding row in the respective embedding layer. Here is an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, we convert the pandas dataframe into a torch.tensor. We check the size, which is 1460x5, which is the same as of the original dataframe with the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_cat = torch.tensor(df_train[cat_names].values, dtype=torch.long, device='cpu')\n",
    "x_cat, x_cat.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first row contains the values [0, 0, 0, 5, 3].\n",
    "We will now get the rows\n",
    "\n",
    "* 0 from emb_layer0\n",
    "* 0 from emb_layer1\n",
    "* 0 from emb_layer2\n",
    "* 5 from emb_layer3\n",
    "* 3 from emb_layer4\n",
    "\n",
    "and concatenate them to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# x = [emb_layer(cat_data[:, i]) for i,emb_layer in enumerate(self.emb_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x = [emb_layer(x_cat[0, i]) for i,emb_layer in enumerate(emb_layers)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.cat(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we can see, we now have 14 values. 1 from emb_layer0, 1 from emb_layer1, 1 from the emb_layer2, 8 from emb_layer3 and 5 from emb_layer4. The values correspond to the weigths in the respective embedding layer.\n",
    "\n",
    "Of course we can also do that for all rows at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [emb_layer(x_cat[:, i]) for i, emb_layer in enumerate(emb_layers)]\n",
    "x = torch.cat(x, 1)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the neural network, this tensor will be concatenated to the tensor that holds the continuous variables, and will serve as input matrix for the first linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = LinearNet(emb_dims=emb_dims, num_cont=len(cont_names)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate and optimizer\n",
    "lr = 0.1\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "def fit(num_epochs, model, loss_fn, opt):    \n",
    "    for epoch in range(num_epochs):\n",
    "        for xb_cat, xb_cont, yb in train_dl:\n",
    "            # Forward\n",
    "            #xb_cat, xb_cont, yb = xb_cat.to(device), xb_cont.to(device), yb.to(device)\n",
    "            preds = model(xb_cat, xb_cont)\n",
    "            loss = loss_fn(preds, yb)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Gradient descent\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        if epoch%20==0:\n",
    "            print('Training loss:', loss_fn(model(t_train_cat, t_train_cont), t_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train for 300 epochs\n",
    "fit(num_epochs=300, model=model, loss_fn=loss_fn, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(t_train_cat, t_train_cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([preds, t_y.reshape(-1,1)], dim=1)[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding categorical variables and embeddings has helped us to to decrease the loss further, by approximately 20% (2.2 vs 1.8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(preds.detach().cpu().numpy(), t_y.reshape(-1,1).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
